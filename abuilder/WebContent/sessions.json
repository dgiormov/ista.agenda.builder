[{
  "id": 11,
  "name": "The AngularJS way",
  "type": "hands-on",
  "speakers": [
    {
      "id": "bmihaylov"
    }
  ],
  "room": "Track A",
  "track": "Track A",
  "date": "26.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1300,
  "startTimeString": "13:00",
  "duration": 120,
  "isSelected": false,
  "description": "Every day a new JavaScript library or framework is born. Seriously. AngularJS is however one of the leaders when talking about single-page applications (SPAs). In this talk I will present AngularJS in terms of problems and the solutions it provides to them. I will discuss how traditional concepts of communication between the browser and the server (f.x. data transfer, state management, and user authentication) can be solved using the framework. Furthermore, I will dive into typical pitfalls when developing applications using AngularJS and how one can avoid them.",
  "hasSecret": false
},
{
  "id": 12,
  "name": "Multi-platform mobile application test automation",
  "type": "hands-on",
  "speakers": [
    {
      "id": "dddimitrov"
    }
  ],
  "room": "Track B",
  "track": "Track B",
  "date": "26.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1300,
  "startTimeString": "13:00",
  "duration": 120,
  "isSelected": false,
  "description": "We will present process, frameworks and tools to accomplish fully automated test execution against Android and iOS, covering physical devices as well as emulators/simulators.  Single Java test code is executed against all platforms.
   A sample mobile application which connects to a remote server will be demonstrated. The test suite executes tests against the mobile application as well manipulates data on the remote server.
Topics covered in the presentation:
1.	One test code, executed on several platforms
2.	Execution against real devices and emulators
3.	Simulating user interactions on the application GUI
4.	Parallel/simultaneous execution of test suites
5.	File management and verification on devices
6.	Operations and verifications on any remote server Operating System 
7.	Storage, visualization and reporting of test results
8.	Full integration with Jenkins CI",
  "hasSecret": false
},
{
  "id": 13,
  "name": "Transform automated webservice test framework into a fuzzy engine for probing logical flaws",
  "type": "hands-on",
  "speakers": [
    {
      "id": "mgag"
    }, {
      "id": "ghegdal"
    }
  ],
  "room": "Track C",
  "track": "Track C",
  "date": "26.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1300,
  "startTimeString": "13:00",
  "duration": 120,
  "isSelected": false,
  "description": "Brief Description:
Fuzz testing or fuzzing is a software testing technique, but it can nonetheless reveal important bugs in your programs. It can identify real-world failure modes and signal potential avenues of attack that should be plugged before your software ships, often automated or semi-automated, that involves providing invalid, unexpected, or random data to the inputs of a computer program. The program is then monitored for exceptions such as crashes, or failing built-in code assertions or for finding potential memory leaks. Fuzzing is commonly used to test for security problems in software or computer systems.  
 
“Fuzzing is useful for finding bugs in bad code. The number-one mistake application developers make in testing is that they expect data to arrive in a certain order and fuzzing can get round this. But the trick is to know when to stop fuzzing and how to move on to other techniques such as static analysis,” (Brian Chess, chief scientist at Fortify Software and also at RSA,Ref :http://www.theregister.co.uk/Print/2008/04/07/fuzzing_advice/) 

We have automated the process of fuzzing web services API by chaining them with regular conventional API test programs which use engines like JAX-WS/ AXIS in accessing a remote object via stubs with SOAP (simple object access protocol )communications, and typically used for functional tests. We fuzz them by attaching mutation templates that would parse and mutate the actual SOAP XML prior it reaches the server, at any point in web transaction.
 
This fuzzing technique induces the server to expose lot of its unknown vulnerabilities like Buffer overflows that would lead to crashing or DoS (Denail of service attacks ).
 
The Mutation template is provided with the option injecting / embedding a CDATA/XSS/SQL injections/ XML Schema attacks , all this to happen in a automated fashion either sequentially or in parallel .And further expecting the target to throw proper faults at right time , if not then its considered as a bug .
 
Advantage of this technique is QA folks can reuse their existing API test cases to make them as a data generating engine by sniffing the SOAP body of the specific API they are interested to fuzz at any point in time , which can then be used to feed the fuzz engine to further mutate the SOAP function calls and sending it to the target , this saves lot of data modelling work  which otherwise required, when we use the commercial fuzzing client tools like SOAPUI / WSFuzzer /Peach etc .
  
Following this technique early in the development process would help early detection of possible security loop holes , logical flaws ensuring the robustness of the product software more robust.
  
 Outline of the Presentation:
  
 1.  Below would be the outline of the presentation,
 2. Overview of the web services fuzzing techniques
 3. Current approaches used in web services fuzzing
 4. How to convert and couple the existing API test ware as a fuzzing tool.
 5. Details on automated fuzzing framework
 6. Demo
  
 ·        Sample Code review
  
 ·        POC – with the experiment carried out on vSphere product",
  "hasSecret": false
}
,{
  "id": 14,
  "name": "Evaluate testing efficiency using Jacoco Code Coverage tool",
  "type": "hands-on",
  "speakers": [
    {
      "id": "hgergov"
    }
  ],
  "room": "Track A",
  "track": "Track A",
  "date": "26.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1500,
  "startTimeString": "15:00",
  "duration": 120,
  "isSelected": false,
  "description": "Quality engineers are facing a great challenge when deciding how much testing is enough before releasing a new feature or how many testing resources are needed in order to release a product that meets client expectations. Of course most of them just do some guesses based on experience or impact analysis, but the truth is that technically those issues cannot be addressed correctly every time. Big-scale software needs big-scale testing: Automation, Integration, Manual, Performance, Unit and etc. Most of the time those different types of testing overlap in some functionalities while other components of the system are not tested at all. Another problem in testing big-scale software is capacity: some features are easier and cheaper to test on a component level (Unit Testing) while for others System Testing is much more appropriate. Of course the developers could take off some of the burden by having a lot of code coverage from their Unit tests, but some Unit tests are just not worth writing at all.  The more test cases and types of tests are introduced, the harder it is to make informative decisions and estimate testing resources properly.
Here on help comes a new tool for Java Code Coverage measurement that is emerging the last couple of years called Jacoco. The feature that makes Jacoco the better choice when evaluating code coverage tools is its ability to instrument Binaries. This means that with this tool you cannot only measure code coverage for Unit Tests but it is also possible to measure the code coverage of testing that is executed using the UI of the application and all other testing activities that are done post-build. This way it can unify all testing results into a common graphic that will display exactly which line of code is not executed at all during all kinds of test execution thus giving invaluable information for decision making during the whole development lifecycle.
	A demonstration of using Jacoco in a Continuous Improvement process will be included in the presentation. It will cover building a project with an instrumented Java Virtual machine, execution of different types of tests and evaluation of the code coverage for each of them. At the end some graphic will be presented that will give information about the code coverage percent by each type of test and a unified graphic that will show code coverage for all types for testing. It will be demonstrated how Jacoco can measure which exact line of code is not covered by any test, i.e. which functionality is not tested at all."
,
  "hasSecret": false
},{
  "id": 15,
  "name": "Testing SWT (Eclipse Plug-ins) with QF-Test",
  "type": "hands-on",
  "speakers": [
    {
      "id": "isamardjiev"
    },
	{
      "id": "gstaykova"
    }
  ],
  "room": "Track B",
  "track": "Track B",
  "date": "26.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1500,
  "startTimeString": "15:00",
  "duration": 120,
  "isSelected": false,
  "description": "I will first present our problem and how we chose to use QF-Test - what its advantages and disadvantages over other solutions are and how they apply to us. 
I will then give a short description of how it works and how it deals with Eclipse and its objects.
Then I will present how generally tests are organized, how they are executed and how reports look and debugged.
Finally I will show how we’ve integrated in Jenkins and give a short demo with Eclipse.",
  "hasSecret": false
},
{
  "id": 16,
  "name": "Mobile Testing in the Cloud",
  "type": "hands-on",
  "speakers": [
    {
      "id": "djambov"
    }
  ],
  "room": "Track C",
  "track": "Track C",
  "date": "26.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1500,
  "startTimeString": "15:00",
  "duration": 120,
  "isSelected": false,
  "description": "Platform independent free automation framework Java Script based, for testing both desktop and mobile devices (android, iOS) for Web and Native applications, integration of this framework with other platforms and products for test organization and analysis and reporting of the results.
",
  "hasSecret": false
},
{
  "id": 17,
  "name": "Agile software development techniques for daily use. What to do when the sprint starts?",
  "type": "hands-on",
  "speakers": [
    {
      "id": "ndokovski"
    }
  ],
  "room": "Track A",
  "track": "Track A",
  "date": "26.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1700,
  "startTimeString": "17:00",
  "duration": 120,
  "isSelected": false,
  "description": "The session will introduce a set of software development techniques used today by agile teams. The topic covers parctices such as Test Driven Development, Pair Programming, Agile Test Quadrant, Clean Code, etc.  In addition to the introduction of the techniques the session will present examples, experiences and observations in applying those techniques in large scale software development projects and organizations. By the end of the session, participants will have strong foundation of practices which can by applied in current or future software development projects.",
  "hasSecret": false
},
{
  "id": 18,
  "name": "Testing Applications Using Apache Camel",
  "type": "hands-on",
  "speakers": [
    {
      "id": "rspasov"
    }
  ],
  "room": "Track B",
  "track": "Track B",
  "date": "26.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1700,
  "startTimeString": "17:00",
  "duration": 120,
  "isSelected": false,
  "description": "First I plan to give a short (1-2 slides) introduction of Apache Camel for the ones that have missed my other presentation called \"Integration Made Easy With Apache Camel\". After that using the example from that first session, I will show how developers and quality engineers can test it using the helper libraries available  in the framework, described here: http://camel.apache.org/testing.html.
Here's a short description of the presentation as I currently foresee it:
1. A short overview of Apache Camel consisting of 1 or 2 slides for those that have not attended my first presentation introducing Camel to the wider audience.
2. An overview of the libraries and modules that come out-of-the-box with the framework and can be used to make the life of people testing applications based on Apache Camel a lot easier. They are described here: http://camel.apache.org/testing.html.
3. Using the example from the first presentation, I will show how people can write both unit and integration tests using those helper libraries provided by the framework. I will show them how to mock endpoints, alter routes during test execution and verify expected results: http://camel.apache.org/camel-test.html
4. I will expand the test scenario so it uses Spring and Camel's XML DSL: http://camel.apache.org/spring-testing.html
5. If time allows I will further expand the example so it runs under Apache Karaf (an OSGi implementation) using Blueprint and modify the test so it runs using Blueprint as well: http://camel.apache.org/blueprint-testing.html
6. At the end I will finish with a short summary of the best practices I've learned in using Camel and testing applications based on it.",
  "hasSecret": false
},
{
  "id": 19,
  "name": "Walkman: Never write another Unit Test. Just Generate",
  "type": "hands-on",
  "speakers": [
    {
      "id": "dsathyamurthy"
    }
  ],
  "room": "Track C",
  "track": "Track C",
  "date": "26.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1700,
  "startTimeString": "17:00",
  "duration": 120,
  "isSelected": false,
  "description": "Good testing practices lead to development of good code. Unit testing is an efficient way to achieve this goal. Though developers spend considerable amount of time in manually testing various scenarios and boundary conditions, automation of these test cases is hardly achieved as mocking/building test data for unit test cases is considered time and effort-consuming. To ensure we have better quality products by virtue of having better unit tests, it is important to create an easy way of writing and maintaining unit test cases.
In this paper, we propose a solution where we record data during all the manual test scenarios and generate corresponding unit test cases, using Spring, Aspect Oriented Programming (AOP), Mocking, and Serialization/Deserialization frameworks. We ask the developer to identify the Class(es) to be tested, and generate proxy code that will surround and record all external interactions of the class into a seed file. The application is then deployed with these proxies and manually tested for all scenarios. During this, recording takes place into seed files. After testing, the developer collects these seed files, and provides it to the framework, which generates unit test cases for the class under test by mocking the interactions as recorded in the seed files.
Thus, we propose a “record-manual, replay-automated” framework that can extract unit test cases during existing process of manual testing, without any extra effort. . By experience of developers who write mock based test cases, it would be fair to say 80% of the effort in writing test cases is in imagining and cooking mock data, and 20% in Asserts. While record replay frameworks exist at UI automation levels, they capture user interaction and replay on a fully configured (with database, message bus, other products etc.), running system in a preset configuration. This does not help in creating unit test cases since unit tests will not have (not supposed to have!) a fully configured system at their expense. So there is a good scope for improving the 80% effort of creating mock data, when writing mock based test cases.
In this paper we propose a solution that will reduce the effort ratio of writing unit test cases by as much as 90% and mocking time to near zero.

The solution works in three phases. The first phase generates proxies or the interceptor code for the classes identified for testing. We use Spring-AOP for proxy mechanism. The second phase involves deploying the app along with interceptor code and running manual test scenarios that invoke the changed classes. This phase generates the seed files that capture all the interactions of the classes under test and their dependencies. We have chosen Json to represent this data in seed files. The third phase is collecting the seed files and generating the Unit tests. We have used JcodeModel to generate Mockito based mocking unit tests.

The solution has the following advantages:
1.	Extraction of unit test cases with minimal effort: Since the process of manual testing is anyway always performed, the developer is putting in no extra effort for writing unit test cases, but is achieving easy repeatability.
2.	Accurate creation of test data: It is not uncommon to write a test case with all required data and debug it for a day on why is it failing, just to realize that the test data that was provided had issues. This solution completely avoids such situation as the test data provided is actually recorded from real scenarios
3.	Maintainability of test cases: The generated test cases are like any other hand written Junit test case. In case there are minor code changes, a regeneration is not always nec-essary. Updating the test case is as easy as editing any source file. 
4.	Works without modifying any original code.

Prior art:
Google code pro: Google codepro has eclipse plugins which statically analyze code to “Guess” test data. It generates full test cases with multiple mutations of test data. However, in most cases guessed data is not sufficient. Additionally, for complex input structures (say Triangle bean with s1, s2 and s3 and side lengths), combination of data values that are business relevant are almost impossible to guess with static code analysis. In contrast, Walkman captures data during user testing which is bound to be business relevant data.

EasyCoverage: They automate the creation of repetitive asserts in test case and not the painful part of mocking data for unit tests.

AppPerfect : They primarily target integration test cases by recording inputs that are coming from a JSP in properties file then replaying it on real code. However, such test cases generated have the same problems of integration tests. Every external system (say a message bus) in the codepath of execution has to be mocked or configured. Walkman on the other hand generates pure Unit test cases whose flow do not go beyond the target class.

Many other tools such as JUB claim automatic test case generation, mean generating boiler plate code with empty test methods which have to be filled by developers. 
Many other efforts exist which follow the lines of either codepro to perform static analysis of the code or work at integration test levels recording inputs from specific sources only such as web page and database.
We propose a solution here that will ease out the task of creating unit test cases for most of the Java web application developers. This can result in big productivity gain since the process of writing unit tests is much simpler and faster. When the solution is adapted, the product quality automatically goes up since we will start seeing more and more test cases in the system.",
  "hasSecret": false
},
{
  "id": 101,
  "name": "JavaScript Performance Analysis and Testing by Instrumentation",
  "type": "session",
  "speakers": [
    {
      "id": "atodorov"
    }
  ],
  "room": "Track A",
  "track": "Track A",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1000,
  "startTimeString": "10:00",
  "duration": 60,
  "isSelected": false,
  "description": "Analyzing web apps performance is a challenging topic, and there are few companies that do it in an unattended, repeatable way. I’ve always wanted to automated those performance measurements, similar to how I can do it using Chrome Dev Tools and Firebug; moreover, I’d like this to work on any platform (in browser, Node.JS); I would also like to achieve some easy and flexible reporting around my perf results – I would like to visualize and distribute the data in any way I want.
I’m going to show you a novel way of measuring JavaScript performance by making use of instrumentation and dynamic code tree injection. This applies to both web apps, as well as pure Node.JS apps. It can virtually apply to any piece of JavaScript code. Parts of the concepts are implemented in a framework of mine which I’m going to demonstrate. I am also making that framework available as an open source project on github.com",
  "hasSecret": false
},
{
  "id": 102,
  "name": "Extreme Programming",
  "type": "session",
  "speakers": [
    {
      "id": "nbogdanov"
    }
  ],
  "room": "Track B",
  "track": "Track B",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1000,
  "startTimeString": "10:00",
  "duration": 60,
  "isSelected": false,
  "description": "At first I will show some basics and will tell something more in theory as introduction. After that the main idea is to have agile presentation which will be developing to the end of the session. Following different games and giving the audience the power to interact with me, I will develop the presentation in the most beneficial for the audience way.",
  "hasSecret": false
},
{
  "id": 103,
  "name": "Animate and Automate",
  "type": "session",
  "speakers": [
    {
      "id": "dtotseva"
    },
	{
      "id": "givanov"
    }
  ],
  "room": "Track C",
  "track": "Track C",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1000,
  "startTimeString": "10:00",
  "duration": 60,
  "isSelected": false,
  "description": "This session is about techniques for automatically test CSS animations, 3D transforms and tracking HTML5 video executions on mobile devices, directly connecting to NoSQL Big Data solution. Using analyses and automation approach to successfully determine runtime performance, events, cross browser compatibility and how smooth is the animation. Discuss how to select cloud based solution and to overcome the necessity of testing on multiple devices. Measure performance display and prove that the user experience segment has been improved with the help of CSS.",
  "hasSecret": false
},
{
  "id": 111,
  "name": "Get prepared for the future with “Testing of Single Page Web Applications”",
  "type": "session",
  "speakers": [
    {
      "id": "rlönn"
    }
  ],
  "room": "Track A",
  "track": "Track A",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1100,
  "startTimeString": "11:00",
  "duration": 60,
  "isSelected": false,
  "description": "I want to share my experience with building single page applications and the specific challenges we've had in the development and testing process.
Single page application are becoming very common these days, as more and more services that used to be desktop based are moved to the web - ex. administration panels, on-line games, SaaS, etc. - and even entire operating systems are being built with the purpose of working with web applications (ChromeOS), or entirely with web technologies (FirefoxOS).
Because of their special nature, SPAs differ a lot from the standard web page. Standard web testing techniques and tools do not work - automation, performance and SEO bring a lot of challenges and require a different mind set. Selenium won't do the job for automation and performance tools like YSlow! and PageSpeed just won't give you the correct results. There are also some special testing needs, that did not exist in the normal web pages - ex. testing for memory and CPU leaks and performance degradation overtime.
My presentation would cover how to tackle, automate and overcome all these problems and succeed in building and testing good single page applications.",
  "hasSecret": false
},
{
  "id": 112,
  "name": "Soft Performance: Messaging",
  "type": "session",
  "speakers": [
    {
      "id": "pmanev"
    }
  ],
  "room": "Track B",
  "track": "Track B",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1100,
  "startTimeString": "11:00",
  "duration": 60,
  "isSelected": false,
  "description": "Error messages, system messages, status messages, informational messages, warning messages, feedback messages, inline messages…
Interactions with websites, applications, and devices are peppered with messages - the things communicate with their users. Sometimes people understand the messages, sometimes they remain wondering. There are cases they do not even notice a message was shown. In other occasions they feel frustrated, amused, or mad. Violence against the machine might occur.
This talk will take a look at messages and will:
- Discuss the points of view of users, developer s, designers, and businesses.
- Show examples.
- Give practical pieces of advice on writing messages: how and whether or not to.",
  "hasSecret": false
},
{
  "id": 113,
  "name": "Automating the testing of Charts",
  "type": "session",
  "speakers": [
    {
      "id": "iivanov"
    },
	{
      "id": "gpeshterski"
    }
  ],
  "room": "Track C",
  "track": "Track C",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1100,
  "startTimeString": "11:00",
  "duration": 60,
  "isSelected": false,
  "description": "We will start with an introduction of the problem - why do we need to test charts and how ineffective that is when done manually. Then we will look into the options for automation and present a small library called “Chart Recognition Library” (CRE). The CRE library can do OCR-like parsing of different charts and return chart values presented in alphanumeric format.
We will talk about the common problems that you can face when implementing *any* chart recognition library, and show some ways to resolve them. We will also demonstrate a way of using that library for end-to-end and data correctness testing.",
  "hasSecret": false
},
{
  "id": 121,
  "name": "Past the horizon with evolutionary testing",
  "type": "session",
  "speakers": [
    {
      "id": "dmdimitrov"
    }
  ],
  "room": "Track A",
  "track": "Track A",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1200,
  "startTimeString": "12:00",
  "duration": 60,
  "isSelected": false,
  "description": "The presentation will focus on introducing the concepts and possible applications of evolutionary testing techniques in the field of software development and quality assurance. Beside the fields of application the presentation will touch on the pros and cons of using evolutionary testing in practice.
As part of the presentation a practical case study of evolutionary testing application will be demonstrated. The case study details will be presented in the form of design-implementation-results sequence and will be based on real-world application deployment. Due to the specifics of such testing the demonstration most probably won’t be a live one, but will provide as rich detail on the process to illustrate it.",
  "hasSecret": false
},
{
  "id": 122,
  "name": "Automation Testing Legacy Applications - The Good, The Bad, and the Ugly",
  "type": "session",
  "speakers": [
    {
      "id": "eslavov"
    }
  ],
  "room": "Track B",
  "track": "Track B",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1200,
  "startTimeString": "12:00",
  "duration": 60,
  "isSelected": false,
  "description": "Everyone loves to work on a greenfield project. You can choose language, architecture, design from scratch, all the new sexy stuff. The reality is that software engineers at some point need to work on or support legacy applications. It’s alive because it has some business value. The code is so messed up (“I just need to add one more ‘if’ to this method”), and the whole thing is held together by spit and baling wire. There are no automated tests safety net. Every change requires manual tests and a prayer to the software gods. This talk will show you how to start small and work your way up to the point where you reach a confident state. It will show you how to optimise for the team happiness. It will affect topics such as, unit testing, acceptance tests, static code analysis, continuous integration, architecture for testability. The talk is inspired by real life experience, working on three legacy projects in the span of more than four years.",
  "hasSecret": false
},
{
  "id": 123,
  "name": "Delivering PaaS",
  "type": "session",
  "speakers": [
    {
      "id": "barnaudov"
    }
  ],
  "room": "Track C",
  "track": "Track C",
  "date": "27.11.2014",  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1200,
  "startTimeString": "12:00",
  "duration": 60,
  "isSelected": false,
  "description": "The world is changing faster and faster. So are to be the solutions we use. No wonder the Cloud is increasingly popular and becoming the de facto standard for new services. We all use it, we all expect it to be fast, reliable, and feature rich. 
Building a convenient environment in the Cloud for development and hosting of our solutions is a challenging task. All requirements of a complex software projects are in, plus the Cloud specifics. 
In this session we will share with you our experience in building, validating and delivering a Platform-as-a-Service(PaaS). In the last years we established steady and frequent update cycle and we keep the high standards our customers are used to. We will walk you through the whole process from code change to productive feature. We will share the challenges we faced and the solutions we found, in order to have stable and reliable testing procedures allowing us to update all productive landscapes of SAP HANA Cloud Platform each and every two weeks.",
  "hasSecret": false
},{
  "id": 131,
  "name": "Optimized Design Solutions for Test Automation Frameworks",
  "type": "session",
  "speakers": [
    {
      "id": "iduevski"
    }
  ],
  "room": "Track A",
  "track": "Track A",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1400,
  "startTimeString": "14:00",
  "duration": 60,
  "isSelected": false,
  "description": "In quality engineering practice, state-of-the-art architecture and design of test automation frameworks is often downplayed in favor of more mission-critical tasks. However, raising the bar for automation framework design can be decisive for the performance of test automation against key measures for QE teams and the entire QE organization, such as test coverage; accessibility and efficiency of automation development, execution, and maintenance; reusability and deduplication of automation efforts inside and between teams. Optimizing a test framework for those benefits faces specific usability requirements and technical obstacles, which leave the field open to non-standard and novel design solutions. Based on developing the Flex UI automation framework, we shall outline several novel approaches to design and implementation of test frameworks, provide the motivation behind, and demonstrate their potential to improve the return-of-investment from automation:
• Minimizing automation development effort:
(i) Automating Automation - аn integrated framework API design
(ii) The Right Tools: Reinventing design conventions and programming mechanics from the modern object-oriented world
• Maximizing the leverage of prior investments in automation:
(i) Just Plug It In
(ii) Just Make It Work: Design patterns for handling the “not designed for reusability” problem.
(iii) Oops, We (Don’t) Need to Fix Their Code: Employing an instrumentation approach to modify program logic in compiled Java modules without modifying the source code",
  "hasSecret": false
},
{
  "id": 132,
  "name": "Autonomous Software Systems",
  "type": "session",
  "speakers": [
    {
      "id": "kbaylov"
    }
  ],
  "room": "Track B",
  "track": "Track B",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1400,
  "startTimeString": "14:00",
  "duration": 60,
  "isSelected": false,
  "description": "Today system requirements are changing so frequently that developers are hardly managing to satisfy them. Isn't there a way of making systems take critical decisions  themselves and self-adapt towards user needs?
Yes, autonomous software systems are there to solve this problem. They can analyse the surrounding environment and apply changes on a self-based manner without any need of human interaction. In this presentation describe autonomous systems as:
1. What autonomous software systems are
2. How do design them - critical design decisions and dimensions
3. Examples of autonomous systems
4. Future research and development of autonomous systems",
  "hasSecret": false
},
{
  "id": 133,
  "name": "To SPA or not to SPA?",
  "type": "session",
  "speakers": [
    {
      "id": "bmihaylov"
    }
  ],
  "room": "Track C",
  "track": "Track C",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1400,
  "startTimeString": "14:00",
  "duration": 60,
  "isSelected": false,
  "description": "Single-page applications (SPA) continue to break the ice of the traditional web applications. Some companies are building all their projects as SPAs without even thinking about possible setbacks. Others (including developers, UX, and also the customers) do not really see the advantages of this new concept and are scared to implement it. In this talk I will try to provide more evidence in what SPAs actually are, why there are here today and how we can use the best parts of what they promise us. I will begin with a short historical overview of web applications and then will talk about pros & cons of SPAs. The most important part will be about when to use them and when not to use them. Although it is subjective to a certain degree, this will give companies a better perception of what they can achieve with this web concept. If time permits, I will briefly talk about how SPAs work together with search-engine optimization and page tracking systems (e.g., Google)",
  "hasSecret": false
},
{
  "id": 141,
  "name": "Automate Vulnerability Detection in Dependencies",
  "type": "session",
  "speakers": [
    {
      "id": "dddimitrov"
    }
  ],
  "room": "Track A",
  "track": "Track A",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1500,
  "startTimeString": "15:00",
  "duration": 60,
  "isSelected": false,
  "description": "By using third-party components in applications, organization take on the risk of code they have not developed. Statistics show that 80% of code in a typical enterprise application comes from third-party components, yet these components are rarely tested, scrutinized, or updated after passing functional tests. The Open Web Application Security Project (OWASP) has, for the first time, recognized 'Using Components with Known Vulnerabilities' as one of the top ten most relevant security issues. By using an open-source suite of tools, organizations can quickly and automatically identify known vulnerabilities in third-party components. Demonstrations of these tools will be shown and best-practices for using them will be discussed. Hackers rely on unpatched systems to infiltrate and compromise a system. Protect your organization and it's applications by identifying the threats before the bad guys do.",
  "hasSecret": false
},
{
  "id": 142,
  "name": "Intelligent Regression Test as a Service",
  "type": "session",
  "speakers": [
    {
      "id": "dmilov"
    }
  ],
  "room": "Track B",
  "track": "Track B",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1500,
  "startTimeString": "15:00",
  "duration": 60,
  "isSelected": false,
  "description": "Nowadays when almost all regression tests are automated we have to go one step further and offer the regression testing as a service. As a developer rather than doing manual steps to select and run tests to verify my changes lead to regressions or not, I want just to build my code and request service to do this for me. The idea is to use static code analysis to identify altered code between two builds, then using code coverage data from regression tests to find which ones hit the altered code. Executing the tests that hit the code modifications would check regressions are introduced or not.
The presentation will consist of problem definition concerning the high cost of running all regression tests and the risks of running a predefined subset of regression tests on a regular base. I will go through the approach of using static code analysis and code coverage to select the necessary and sufficient subset of test cases. The approach would be proposed for running as an automated service in any kind of regression testing practices such as pre check-in and post check-in, as well as for any kind of regression tests such as unit tests, integration tests, etc. Finally I will show how we in PowerCLI implemented and use Testing as a Service in our daily work. A short demo of the implementation is an option as well.",
  "hasSecret": false
},
{
  "id": 143,
  "name": "Automated way of API performance measurement at individual code change sets",
  "type": "session",
  "speakers": [
    {
      "id": "ghegdal"
    }
  ],
  "room": "Track C",
  "track": "Track C",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1500,
  "startTimeString": "15:00",
  "duration": 60,
  "isSelected": false,
  "description": "Performance of a product is always an issue that is debated across any domain and especially within the software industry it’s pretty common, since most of the products fail with time factor while performing any critical operations, either when its heavily loaded or when new features or additional functionality gets added to the existing working product, which in turn takes a performance toll with existing functionality. It’s pretty hard to identify or root cause where and what went wrong when a complete software is released. 
 
Here the main intention would be to track down the issue at individual code change sets, by triggering API performance measure tests against impacted features/functionality of that software. 

Based on the learnt performance metrics, a bench mark can be set. Any performance deviations seen would help to identity & narrow down to the actual issue (such as Memory leak, Static/Dynamic buffer overruns etc) and figure out a possible solution quickly.
   
Following this technique early in the Development process would help early detection of possible performance issues of the features at lower cost; additionally it would help make the software more robust.
   
Outline of the Presentation:
 
1. Overview of the performance issues with products
 
2. Current approaches to solve the performance issues
 
3. API Performance measurement technique & its advantages over other techniques
 
4. Details on API Performance measurement automated framework
 
5. Demo
 
~ Sample Code review
 
~ POC – with the experiment carried out on vSphere product",
  "hasSecret": false
},
{
  "id": 151,
  "name": "Formal Methods in QE: from scientific idea to practical engineering",
  "type": "session",
  "speakers": [
    {
      "id": "hbolibekyan"
    },
	{
      "id": "aaghabekyan"
    }
  ],
  "room": "Track A",
  "track": "Track A",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1600,
  "startTimeString": "16:00",
  "duration": 60,
  "isSelected": false,
  "description": "Within software’s life cycle program testing is very important as the quality of specification, design and application have to be proven. Quality of a designed software plays important role - error-free software becomes must-have when it deals with safety-critical systems. Generally verification is a process of ensuring system behaves as intended, it is bug-free, functional correctness is provided. It occupies nearly 60% of development time. Traditional design verification techniques are based on testing and simulation, targeted on detection of bugs presence but not their absence in the system. Being widely used this approach has a number of disadvantages, such as late detection of bugs, selections of test vectors and simulation methods.
In this talk we will consider the use of formal methods in QE processes. Formal methods being founded on mathematical models provide more rigorous approach, prove correctness of a system. By the use of these methods bugs are detected on early stages of software development process and in contrast to traditional verification methods it is proved that system does not have bugs but not only that they are not detected using some simulation techniques. We will demonstrate how to use some of those tools (theorem provers, design by contract, etc.) in enterprise software,  some real life examples will be considered during the talk. We will outline some approaches on integrating software development and formal verification processes in providing error-free systems.",
  "hasSecret": false
},
{
  "id": 152,
  "name": "A Starfleet science officer chats up an Egyptian goddess in a jazz club on the Moon... or Groovy Browser Automation",
  "type": "session",
  "speakers": [
    {
      "id": "pdimitrov"
    }
  ],
  "room": "Track B",
  "track": "Track B",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1600,
  "startTimeString": "16:00",
  "duration": 60,
  "isSelected": false,
  "description": "The presentation aims to showcase problems identified in a real-world project and how they can be leveraged by introducing Spock and Geb. It demonstrates the capabilities of these frameworks and well as highlights the basis of the underlying technologies - Groovy and Selenium WebDriver.
Outline:
I. Introduction:
   1) Introduce previous project experience
   2) Identified problems:
      a) different tooling used by DEV and QA
      b) complex testing tool with high learning curve
      c) lots of semi-organized testing code
      d) hard to maintain and support
II. Main part:
   1) Introduce the Geb and Spock framework in short
   2) Present a demo of the framework
   3) Introduce each of the frameworks components and how they address the identified problems (building bottom-up):
      a) Groovy - only highlights, no in-depth language syntax
      b) Spock - introduce advantages (highly expressive, easy to learn, behavior driven, Groovy based DSL), showcase its syntax and capabilities.
      c) Selenium WebDriver - only highlights, mark advantages and identify shortcomings
      d) Geb - explain how it addresses some of Selenium's shortcomings; showcase its syntax and capabilities
 III. Conclusion",
  "hasSecret": false
},
{
  "id": 153,
  "name": "Integration Made Easy With Apache Camel",
  "type": "session",
  "speakers": [
    {
      "id": "rspasov"
    }
  ],
  "room": "Track C",
  "track": "Track C",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1600,
  "startTimeString": "16:00",
  "duration": 60,
  "isSelected": false,
  "description": "I've been using Apache Camel for over 3 years. I found it is quite useful but apparently not a lot of people know about it or have used it in Bulgaria. With this presentation I'd like to increase the awareness and knowledge on this topic and hopefully inspire more software developers to introduce this framework in their projects thus building a better local community.	In general my presentation will be similar to this one but a bit shorter: http://www.slideshare.net/davsclaus/getting-started-with-apache-camel-javagruppen-copenhagen-april-2014
Here is a short description of the presentation as I currently foresee it:
1. A short introduction to Enterprise Integration Patterns - what is Integration and why is it needed.
2. A high overview of Apache Camel and a short example of how it implements the EIP mentioned above using some well-know patterns using Java DSL and XML DSL.
3. An overview of Apache Camel architecture, components, data formats & expression languages
4. A live demo showing how applications can evolve by using Apache Camel
5. A short overview of Camel's testing capabilities  - this will be just a hint towards the next presentation which is fully focused on testing of applications that are using this framework.",
  "hasSecret": false
},
{
  "id": 161,
  "name": "The power and beauty of android automated testing",
  "type": "session",
  "speakers": [
    {
      "id": "bstrandjev"
    },
	{
      "id": "ppetarcheva"
    }
  ],
  "room": "Track A",
  "track": "Track A",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1700,
  "startTimeString": "17:00",
  "duration": 60,
  "isSelected": false,
  "description": "The presentation will outline and demonstrate different tools and approaches for testing of Android applications:
 * Unit testing for Android - JUnit, Android Unit, Robolectric
 * Integration tests - Android instrumentation, Robotium
 * Black box testing - UIAutomator, Appium
 * Cloud services - Perfecto
In the end of the presentation we will make few speculations with our predictions for the field of mobile automated testing.",
  "hasSecret": false
},
{
  "id": 162,
  "name": "What is the future of gamification ?",
  "type": "session",
  "speakers": [
    {
      "id": "nneychev"
    }
  ],
  "room": "Track B",
  "track": "Track B",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1700,
  "startTimeString": "17:00",
  "duration": 60,
  "isSelected": false,
  "description": "We will show the recent gamification trends, findings and devеlopments.
A glimpse of the gamification future beyond business towards health, environment, climate and other important aspects of human well being.

There will be lots of scientific data, best practices, example of incentive based gamification framework, and a life demo of a gamified conference, like ISTA :)",
  "hasSecret": false
},
{
  "id": 163,
  "name": "Bandwidth doesn't matter: why network delay will always limit web performance and what to do about it",
  "type": "session",
  "speakers": [
    {
      "id": "rlönn"
    }
  ],
  "room": "Track C",
  "track": "Track C",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1700,
  "startTimeString": "17:00",
  "duration": 60,
  "isSelected": false,
  "description": "Many people think more bandwidth is the answer to the question of how to speed up website load time, but due to the request-response nature of the HTTP protocol, page-load time will always be a minimum of the number of items multiplied by the network delay between client and server. Even infinite bandwidth will not make things faster than the fundamental limit dictated by network delay. 

This talk will highlight some of the problems with the current HTTP protocol, like its sensitivity to network delay and how that has affected both web development and browser/client evolution, making people create lots of imaginative workarounds for a protocol that was designed in another era, when web sites and pages were small and simple. 

We will show exactly how network delay affects performance for the \"chatty\" protocol that is HTTP, how browsers try to counter the negative effects by parallelizing downloads, and how that in turn causes problems on the server side and sometimes in the network. 

This presentation will explain the relationship between page load times and network delay, why bandwidth often doesn't matter, and discuss how modern browsers use parallel TCP connections in order to get around the network delay problem and speed up page load time. 

We will also discuss what happens in the future, especially with the introduction of HTTP2, which will totally change the rules, and explore this new version from a performance perspective.",
  "hasSecret": false
},
{
  "id": 1,
  "name": "Keynote",
  "type": "",
  "speakers": [
    {
      "id": ""
    }
  ],
  "room": "Track ",
  "track": "",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 900,
  "startTimeString": "9:00",
  "duration": 60,
  "isSelected": false,
  "description": "",
  "hasSecret": false
},
{
  "id": 2,
  "name": "Lunch",
  "type": "Lunch",
  "speakers": [
    {
      "id": ""
    }
  ],
  "track": "",
  "date": "27.11.2014",
  "room": "Track ",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1300,
  "startTimeString": "13:00",
  "duration": 60,
  "isSelected": false,
  "description": "",
  "hasSecret": false
},
{
  "id": 3,
  "name": "Cocktail",
  "type": "",
  "speakers": [
    {
      "id": ""
    }
  ],
  "room": "Track ",
  "track": "",
  "date": "27.11.2014",
  "searchTerms": "",
  "tags": [
    ""
  ],
  "startTime": 1800,
  "startTimeString": "18:00",
  "duration": 120,
  "isSelected": false,
  "description": "",
  "hasSecret": false
}
]